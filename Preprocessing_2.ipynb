{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-V-XgsU0Wm1"
      },
      "source": [
        "# Environment Setting Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUtz6oet612K",
        "outputId": "0d3560a3-6556-46d4-954c-5a80033de319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Quantmetry/qolmat\n",
            "  Cloning https://github.com/Quantmetry/qolmat to /tmp/pip-req-build-6jldhcgj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Quantmetry/qolmat /tmp/pip-req-build-6jldhcgj\n",
            "  Resolved https://github.com/Quantmetry/qolmat to commit f94aafd284622d67ded261772c8d10de15d025ec\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting category_encoders (from qolmat==0.1.8)\n",
            "  Downloading category_encoders-2.6.4-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting dcor>=0.6 (from qolmat==0.1.8)\n",
            "  Downloading dcor-0.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (from qolmat==0.1.8) (0.2.7)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from qolmat==0.1.8) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from qolmat==0.1.8) (24.2)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from qolmat==0.1.8) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from qolmat==0.1.8) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from qolmat==0.1.8) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.14 in /usr/local/lib/python3.10/dist-packages (from qolmat==0.1.8) (0.14.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qolmat==0.1.8) (4.12.2)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.10/dist-packages (from dcor>=0.6->qolmat==0.1.8) (0.60.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from dcor>=0.6->qolmat==0.1.8) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->qolmat==0.1.8) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->qolmat==0.1.8) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->qolmat==0.1.8) (2024.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14->qolmat==0.1.8) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->qolmat==0.1.8) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt->qolmat==0.1.8) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt->qolmat==0.1.8) (3.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt->qolmat==0.1.8) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt->qolmat==0.1.8) (4.66.6)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt->qolmat==0.1.8) (3.1.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt->qolmat==0.1.8) (0.10.9.7)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51->dcor>=0.6->qolmat==0.1.8) (0.43.0)\n",
            "Downloading dcor-0.6-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: qolmat\n",
            "  Building wheel for qolmat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qolmat: filename=qolmat-0.1.8-py3-none-any.whl size=16447861 sha256=5d1ab5df881992473c3f6d867f0ff0ab896d84045a97d0732c13ba32acb62b34\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6402sj7x/wheels/47/59/9f/6f835741a1c823bf666503f52c9aead157996c064ef495c41a\n",
            "Successfully built qolmat\n",
            "Installing collected packages: dcor, category_encoders, qolmat\n",
            "Successfully installed category_encoders-2.6.4 dcor-0.6 qolmat-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/Quantmetry/qolmat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgNgU-LUwqqt"
      },
      "source": [
        "# Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "5zrn8DHylezi"
      },
      "outputs": [],
      "source": [
        "from qolmat.benchmark import comparator, missing_patterns\n",
        "from qolmat.imputations import imputers\n",
        "from qolmat.utils import plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "dbYmIqAUPj9y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# import scienceplots\n",
        "\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.metrics import *\n",
        "from sklearn.preprocessing import *\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "\n",
        "import missingno\n",
        "import warnings\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V0DgZMo67oT"
      },
      "source": [
        "# Global Configuration Setting Controling Randomness, Trials, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "Jjl2tp-cgypS"
      },
      "outputs": [],
      "source": [
        "sklearn.set_config(transform_output=\"pandas\")\n",
        "np.seterr(under='ignore')\n",
        "warnings.filterwarnings('ignore')\n",
        "SEED = 42\n",
        "n_trials = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wfcbe9Fc1qT"
      },
      "source": [
        "# Read the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "h-TPDu_UdTlC"
      },
      "outputs": [],
      "source": [
        "# reading the train dataset\n",
        "df = pd.read_csv(\"X_train.csv\",\n",
        "                 dayfirst=True,\n",
        "                 parse_dates=True,\n",
        "                 index_col=\"Date\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0Rtj6WgJWhq",
        "outputId": "c8405cec-dc20-47d5-dd51-6210f5cfe418"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['DS of Influent Primary Sludge (%)',\n",
              "       'VS of Influent Primary Sludge (%)', 'DS of Influent Waste Sludge (%)',\n",
              "       'VS of Influent Waste Sludge (%)', 'DS in Digesters (%)',\n",
              "       'DS of effluent Sludge (%)', 'VS of effluent Sludge (%)',\n",
              "       'Alkalinity (mg CaCO3/L)', 'Fatty Acid (mg/L)', 'pH',\n",
              "       'Temperature (Degrees Celsius)',\n",
              "       'Influent Primary to Waste Sludge flowrate Ratio',\n",
              "       'Influent Primary Sludge flowrate (m3/d)',\n",
              "       'Influent Waste Sludge flowrate (m3/d)',\n",
              "       'Total Effluent Sludge flowrate (m3/d)'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkjWv5waM0RS",
        "outputId": "b1a07462-e449-4b91-c3e5-11aa2dbe4126"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(755, 15)"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrJzKmn0dDPy"
      },
      "source": [
        "# Missing Value Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Qs3oKRZ9QAS-"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the training data\n",
        "df_scaling =scaler.fit(df)\n",
        "df_scaled = df_scaling.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K398uVgQ6rL",
        "outputId": "eac0fec3-86fe-494b-e2a1-68e3dfa6faf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing model: Mean...done.\n",
            "Testing model: Median...done.\n",
            "Testing model: K-nearest neighbors...done.\n",
            "Testing model: EM sampler...done.\n",
            "Testing model: interpolation...done.\n",
            "Testing model: LOCF...done.\n",
            "Testing model: TSA...done.\n"
          ]
        }
      ],
      "source": [
        "# UHG\n",
        "median_imputer = imputers.ImputerSimple(strategy=\"median\")\n",
        "mean_imputer = imputers.ImputerSimple(strategy=\"mean\")\n",
        "knn_imputer = imputers.KNNImputer(n_neighbors=3)\n",
        "em_imputer = imputers.ImputerEM(model=\"VAR\", method=\"mle\",max_iter_em=500,\n",
        "                                n_iter_ou=20, dt=1e-3, p=1, random_state=SEED)\n",
        "inter_imputer = imputers.ImputerInterpolation(method=\"linear\")\n",
        "LOCF_imputer = imputers.ImputerLOCF()\n",
        "# Time interpolation and TSA decomposition\n",
        "TSA_imputor = imputers.ImputerResiduals(period=365, model_tsa=\"additive\")\n",
        "\n",
        "\n",
        "\n",
        "dict_imputers = {\n",
        "    \"Mean\": mean_imputer,\n",
        "    \"Median\": median_imputer,\n",
        "    \"K-nearest neighbors\": knn_imputer,\n",
        "    \"EM sampler\": em_imputer,\n",
        "    \"interpolation\": inter_imputer,\n",
        "    \"LOCF\": LOCF_imputer,\n",
        "    \"TSA\": TSA_imputor\n",
        "  }\n",
        "\n",
        "generator_holes = missing_patterns.UniformHoleGenerator(\n",
        "    n_splits=10,\n",
        "    subset=df_scaled.columns,\n",
        "    ratio_masked=0.25)\n",
        "\n",
        "\n",
        "comparison = comparator.Comparator(\n",
        "      dict_imputers,\n",
        "      df_scaled.columns,\n",
        "      generator_holes = generator_holes,\n",
        "      # metrics = [\"mae\", \"wmape\", \"KL_columnwise\", \"energy\"],\n",
        "      metrics = [\"mae\"],\n",
        "  )\n",
        "\n",
        "results = comparison.compare(df_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "dRUCq6iMjUOp"
      },
      "outputs": [],
      "source": [
        "results.to_csv(\"X_train_imputation_results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "pEUNTyGLDjJn"
      },
      "outputs": [],
      "source": [
        "em1_imputer_columns = ['DS of Influent Primary Sludge (%)']\n",
        "knn1_imputer_columns = ['VS of Influent Primary Sludge (%)']\n",
        "\n",
        "em2_imputer_columns = ['DS of Influent Waste Sludge (%)',\n",
        "                       'VS of Influent Waste Sludge (%)']\n",
        "\n",
        "knn2_imputer_columns = ['DS in Digesters (%)']\n",
        "\n",
        "em3_imputer_columns = ['DS of effluent Sludge (%)',\n",
        "                       'VS of effluent Sludge (%)']\n",
        "\n",
        "knn3_imputer_columns = ['Alkalinity (mg CaCO3/L)',\n",
        "                        'Fatty Acid (mg/L)',\n",
        "                        'pH',\n",
        "                        'Temperature (Degrees Celsius)']\n",
        "\n",
        "em4_imputer_columns = ['Influent Primary to Waste Sludge flowrate Ratio']\n",
        "\n",
        "\n",
        "knn4_imputer_columns = ['Influent Primary Sludge flowrate (m3/d)']\n",
        "\n",
        "em5_imputer_columns = ['Influent Waste Sludge flowrate (m3/d)',\n",
        "                       'Total Effluent Sludge flowrate (m3/d)']\n",
        "\n",
        "\n",
        "\n",
        "median_imputer = imputers.ImputerSimple(strategy=\"median\")\n",
        "mean_imputer = imputers.ImputerSimple(strategy=\"mean\")\n",
        "knn_imputer = imputers.KNNImputer(n_neighbors=5)\n",
        "em_imputer = imputers.ImputerEM(model=\"VAR\", method=\"mle\",max_iter_em=50,\n",
        "                                n_iter_ou=15, dt=1e-3, p=1, random_state=SEED)\n",
        "inter_imputer = imputers.ImputerInterpolation(method=\"linear\")\n",
        "LOCF_imputer = imputers.ImputerLOCF()\n",
        "TSA_imputor = imputers.ImputerResiduals(period=365, model_tsa=\"additive\")\n",
        "\n",
        "\n",
        "\n",
        "transformers_for_imputing = [(\"em1_imputer\", em_imputer, em1_imputer_columns),\n",
        "                             (\"knn1_imputer_columns\", knn_imputer, knn1_imputer_columns),\n",
        "                             (\"em2_imputer\", em_imputer, em2_imputer_columns),\n",
        "                             (\"knn2_imputer_columns\", knn_imputer, knn2_imputer_columns),\n",
        "                             (\"em3_imputer\", em_imputer, em3_imputer_columns),\n",
        "                             (\"knn3_imputer_columns\", knn_imputer, knn3_imputer_columns),\n",
        "                             (\"em4_imputer\", em_imputer, em4_imputer_columns),\n",
        "                             (\"knn4_imputer_columns\", knn_imputer, knn4_imputer_columns),\n",
        "                             (\"em5_imputer\", em_imputer, em5_imputer_columns),\n",
        "                             ]\n",
        "\n",
        "column_imputers = ColumnTransformer(transformers_for_imputing).fit(df)\n",
        "\n",
        "\n",
        "df_filled_na = column_imputers.transform(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXejYpQNRVra",
        "outputId": "65ec51d1-de6e-4834-ed8f-9a27fdd38b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "em1_imputer__DS of Influent Primary Sludge (%)                   0\n",
            "knn1_imputer_columns__VS of Influent Primary Sludge (%)          0\n",
            "em2_imputer__DS of Influent Waste Sludge (%)                     0\n",
            "em2_imputer__VS of Influent Waste Sludge (%)                     0\n",
            "knn2_imputer_columns__DS in Digesters (%)                        0\n",
            "em3_imputer__DS of effluent Sludge (%)                           0\n",
            "em3_imputer__VS of effluent Sludge (%)                           0\n",
            "knn3_imputer_columns__Alkalinity (mg CaCO3/L)                    0\n",
            "knn3_imputer_columns__Fatty Acid (mg/L)                          0\n",
            "knn3_imputer_columns__pH                                         0\n",
            "knn3_imputer_columns__Temperature (Degrees Celsius)              0\n",
            "em4_imputer__Influent Primary to Waste Sludge flowrate Ratio     0\n",
            "knn4_imputer_columns__Influent Primary Sludge flowrate (m3/d)    0\n",
            "em5_imputer__Influent Waste Sludge flowrate (m3/d)               0\n",
            "em5_imputer__Total Effluent Sludge flowrate (m3/d)               0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df_filled_na.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "8tuY10JCMF9o"
      },
      "outputs": [],
      "source": [
        "df = df_filled_na"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "xX7zmQF_RfCq"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"XtrainImputed.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2LXpp6F5VDo"
      },
      "source": [
        "# Outlier Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4_cnew5LEFw",
        "outputId": "bd8649e6-75d0-4341-b427-504b25f28270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of outliers detected: 99\n"
          ]
        }
      ],
      "source": [
        "# --- Outlier Removal Using Z-Score Method for Train Data ---\n",
        "from scipy.interpolate import interp1d\n",
        "# Calculate z-scores to identify outliers\n",
        "z_scores = (df - df.mean()) / df.std()\n",
        "\n",
        "# Set a z-score threshold for identifying outliers\n",
        "z_threshold = 3\n",
        "\n",
        "# Identify outliers\n",
        "outliers = (np.abs(z_scores) > z_threshold).any(axis=1)\n",
        "\n",
        "# Create an array of indices\n",
        "indices = np.arange(len(df))\n",
        "\n",
        "# Create an interpolation function for each column\n",
        "interp_funcs = {}\n",
        "for column in df.columns:\n",
        "    interp_funcs[column] = interp1d(indices[~outliers], df.loc[~outliers, column], kind='linear', fill_value='extrapolate')\n",
        "\n",
        "# Replace outliers with interpolated values for each column\n",
        "data_interp = pd.DataFrame({column: interp_funcs[column](indices) for column in df.columns})\n",
        "\n",
        "data_interp.index = df.index\n",
        "\n",
        "df_ro=data_interp\n",
        "\n",
        "df_ro_sum = df_ro.describe().round(2)\n",
        "df_ro_sum.transpose()\n",
        "\n",
        "# Count the number of outliers detected\n",
        "num_outliers = np.sum(outliers)\n",
        "print(f\"Number of outliers detected: {num_outliers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "vp3c80f-J3gF"
      },
      "outputs": [],
      "source": [
        "X_train_Imputed_ROutlier = df_ro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUoK-oGl5VDp"
      },
      "source": [
        "# Saving the Imputed and Outlier Reduced Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "uG1EqpY4LflA"
      },
      "outputs": [],
      "source": [
        "X_train_Imputed_ROutlier.to_csv(\"X_train_Imputed_ROutlier.CSV\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp9ABsZg53R9"
      },
      "source": [
        "# Preprocessing of Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "n67q-F0J5qzt"
      },
      "outputs": [],
      "source": [
        "# reading the test dataset\n",
        "df_test = pd.read_csv(\"X_test.csv\",\n",
        "                 dayfirst=True,\n",
        "                 parse_dates=True,\n",
        "                 index_col=\"Date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "4MTZ8VVu6BHq"
      },
      "outputs": [],
      "source": [
        "df_filled_na_test = column_imputers.transform(df_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7FHo_a96INU",
        "outputId": "a930a1fb-b841-4961-e742-70c229aecfa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "em1_imputer__DS of Influent Primary Sludge (%)                   0\n",
            "knn1_imputer_columns__VS of Influent Primary Sludge (%)          0\n",
            "em2_imputer__DS of Influent Waste Sludge (%)                     0\n",
            "em2_imputer__VS of Influent Waste Sludge (%)                     0\n",
            "knn2_imputer_columns__DS in Digesters (%)                        0\n",
            "em3_imputer__DS of effluent Sludge (%)                           0\n",
            "em3_imputer__VS of effluent Sludge (%)                           0\n",
            "knn3_imputer_columns__Alkalinity (mg CaCO3/L)                    0\n",
            "knn3_imputer_columns__Fatty Acid (mg/L)                          0\n",
            "knn3_imputer_columns__pH                                         0\n",
            "knn3_imputer_columns__Temperature (Degrees Celsius)              0\n",
            "em4_imputer__Influent Primary to Waste Sludge flowrate Ratio     0\n",
            "knn4_imputer_columns__Influent Primary Sludge flowrate (m3/d)    0\n",
            "em5_imputer__Influent Waste Sludge flowrate (m3/d)               0\n",
            "em5_imputer__Total Effluent Sludge flowrate (m3/d)               0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df_filled_na_test.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "iS6HzwIx6Kdy"
      },
      "outputs": [],
      "source": [
        "df_filled_na_test.to_csv(\"XtestImputed.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzVDuQfB6MNj",
        "outputId": "e8c44b93-5c00-4cb2-872b-32cbb27056d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of outliers detected: 39\n"
          ]
        }
      ],
      "source": [
        "# --- Outlier Removal Using Z-Score Method for Test Data ---\n",
        "from scipy.interpolate import interp1d\n",
        "# Calculate z-scores to identify outliers\n",
        "# mean and standard deviation from training data\n",
        "z_scores = (df_filled_na_test - df.mean()) / df.std()\n",
        "\n",
        "# Set a z-score threshold for identifying outliers\n",
        "z_threshold = 3\n",
        "\n",
        "# Identify outliers\n",
        "outliers = (np.abs(z_scores) > z_threshold).any(axis=1)\n",
        "\n",
        "# Create an array of indices\n",
        "indices = np.arange(len(df_filled_na_test))\n",
        "\n",
        "# Create an interpolation function for each column\n",
        "interp_funcs = {}\n",
        "for column in df_filled_na_test.columns:\n",
        "    interp_funcs[column] = interp1d(indices[~outliers], df_filled_na_test.loc[~outliers, column], kind='linear', fill_value='extrapolate')\n",
        "\n",
        "# Replace outliers with interpolated values for each column\n",
        "data_interp = pd.DataFrame({column: interp_funcs[column](indices) for column in df_filled_na_test.columns})\n",
        "\n",
        "data_interp.index = df_filled_na_test.index\n",
        "\n",
        "df_ro=data_interp\n",
        "\n",
        "df_ro_sum = df_ro.describe().round(2)\n",
        "df_ro_sum.transpose()\n",
        "\n",
        "\n",
        "# Count the number of outliers detected\n",
        "num_outliers = np.sum(outliers)\n",
        "print(f\"Number of outliers detected: {num_outliers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "vWGCzLOc6Pv-"
      },
      "outputs": [],
      "source": [
        "X_test_Imputed_ROutlier = df_ro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "JWPbu53A6RGe"
      },
      "outputs": [],
      "source": [
        "X_test_Imputed_ROutlier.to_csv(\"X_test_Imputed_ROutlier.CSV\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
